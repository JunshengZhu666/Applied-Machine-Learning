{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P245 - P 275"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up \n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ensembles\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    \n",
    "# three classifiers\n",
    "from sklearn.ensemble import VotingClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC \n",
    "\n",
    "log_clf = LogisticRegression() \n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC() \n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.904\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "### look at the accuracy_score \n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### soft voting \n",
    "\n",
    "# average over the prob results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.928"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "bag_clf = BaggingClassifier( \n",
    "    DecisionTreeClassifier(), n_estimators = 500,\n",
    "    max_samples = 100, bootstrap = True, n_jobs = -1\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train, y_train) \n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "# test \n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set oob_score = True\n",
    "bag_clf = BaggingClassifier( \n",
    "    DecisionTreeClassifier(), n_estimators = 500,\n",
    "    bootstrap = True, n_jobs = -1, oob_score = True\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train, y_train) \n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.928"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35326087, 0.64673913],\n",
       "       [0.40306122, 0.59693878],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09876543, 0.90123457],\n",
       "       [0.34636872, 0.65363128],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99462366, 0.00537634],\n",
       "       [0.98265896, 0.01734104],\n",
       "       [0.81714286, 0.18285714],\n",
       "       [0.        , 1.        ],\n",
       "       [0.77941176, 0.22058824],\n",
       "       [0.80327869, 0.19672131],\n",
       "       [0.95675676, 0.04324324],\n",
       "       [0.04395604, 0.95604396],\n",
       "       [0.00571429, 0.99428571],\n",
       "       [0.98816568, 0.01183432],\n",
       "       [0.93582888, 0.06417112],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [0.04191617, 0.95808383],\n",
       "       [0.36094675, 0.63905325],\n",
       "       [0.88888889, 0.11111111],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96373057, 0.03626943],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.59116022, 0.40883978],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.12834225, 0.87165775],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.35135135, 0.64864865],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.22905028, 0.77094972],\n",
       "       [0.35955056, 0.64044944],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02272727, 0.97727273],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01507538, 0.98492462],\n",
       "       [0.99468085, 0.00531915],\n",
       "       [0.88888889, 0.11111111],\n",
       "       [0.96491228, 0.03508772],\n",
       "       [0.99009901, 0.00990099],\n",
       "       [0.00578035, 0.99421965],\n",
       "       [0.06111111, 0.93888889],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76804124, 0.23195876],\n",
       "       [0.38586957, 0.61413043],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.71910112, 0.28089888],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.87373737, 0.12626263],\n",
       "       [1.        , 0.        ],\n",
       "       [0.62827225, 0.37172775],\n",
       "       [0.11557789, 0.88442211],\n",
       "       [0.64324324, 0.35675676],\n",
       "       [0.91208791, 0.08791209],\n",
       "       [0.        , 1.        ],\n",
       "       [0.12135922, 0.87864078],\n",
       "       [0.9112426 , 0.0887574 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03428571, 0.96571429],\n",
       "       [0.0273224 , 0.9726776 ],\n",
       "       [0.30526316, 0.69473684],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85638298, 0.14361702],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.24210526, 0.75789474],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96938776, 0.03061224],\n",
       "       [0.77222222, 0.22777778],\n",
       "       [0.01052632, 0.98947368],\n",
       "       [1.        , 0.        ],\n",
       "       [0.15183246, 0.84816754],\n",
       "       [0.62176166, 0.37823834],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04624277, 0.95375723],\n",
       "       [0.49726776, 0.50273224],\n",
       "       [1.        , 0.        ],\n",
       "       [0.015     , 0.985     ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21311475, 0.78688525],\n",
       "       [0.51724138, 0.48275862],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03370787, 0.96629213],\n",
       "       [0.98507463, 0.01492537],\n",
       "       [0.32663317, 0.67336683],\n",
       "       [0.92696629, 0.07303371],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.82587065, 0.17412935],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00505051, 0.99494949],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99435028, 0.00564972],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00502513, 0.99497487],\n",
       "       [0.96373057, 0.03626943],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00492611, 0.99507389],\n",
       "       [0.20689655, 0.79310345],\n",
       "       [0.96391753, 0.03608247],\n",
       "       [0.28042328, 0.71957672],\n",
       "       [0.98795181, 0.01204819],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01657459, 0.98342541],\n",
       "       [0.70621469, 0.29378531],\n",
       "       [0.44385027, 0.55614973],\n",
       "       [0.37765957, 0.62234043],\n",
       "       [0.85795455, 0.14204545],\n",
       "       [0.9144385 , 0.0855615 ],\n",
       "       [0.04022989, 0.95977011],\n",
       "       [0.77472527, 0.22527473],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04166667, 0.95833333],\n",
       "       [0.98445596, 0.01554404],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00561798, 0.99438202],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00568182, 0.99431818],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97191011, 0.02808989],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.37640449, 0.62359551],\n",
       "       [0.28409091, 0.71590909],\n",
       "       [0.00497512, 0.99502488],\n",
       "       [0.        , 1.        ],\n",
       "       [0.3190184 , 0.6809816 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98941799, 0.01058201],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98333333, 0.01666667],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01442308, 0.98557692],\n",
       "       [0.69680851, 0.30319149],\n",
       "       [0.91620112, 0.08379888],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99468085, 0.00531915],\n",
       "       [0.99404762, 0.00595238],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00520833, 0.99479167],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.08064516, 0.91935484],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02824859, 0.97175141],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04712042, 0.95287958],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93888889, 0.06111111],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [0.57647059, 0.42352941],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14772727, 0.85227273],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94886364, 0.05113636],\n",
       "       [0.96954315, 0.03045685],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00564972, 0.99435028],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48387097, 0.51612903],\n",
       "       [0.82941176, 0.17058824],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98984772, 0.01015228],\n",
       "       [0.01507538, 0.98492462],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96174863, 0.03825137],\n",
       "       [0.        , 1.        ],\n",
       "       [0.23809524, 0.76190476],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98214286, 0.01785714],\n",
       "       [0.84491979, 0.15508021],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10674157, 0.89325843],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03723404, 0.96276596],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02234637, 0.97765363],\n",
       "       [1.        , 0.        ],\n",
       "       [0.84242424, 0.15757576],\n",
       "       [0.        , 1.        ],\n",
       "       [0.88372093, 0.11627907],\n",
       "       [0.98305085, 0.01694915],\n",
       "       [0.20212766, 0.79787234],\n",
       "       [0.20430108, 0.79569892],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.21134021, 0.78865979],\n",
       "       [0.96236559, 0.03763441],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.45177665, 0.54822335],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0994152 , 0.9005848 ],\n",
       "       [0.0867052 , 0.9132948 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.34825871, 0.65174129],\n",
       "       [0.09189189, 0.90810811],\n",
       "       [0.52840909, 0.47159091],\n",
       "       [0.58602151, 0.41397849],\n",
       "       [0.00507614, 0.99492386],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.68205128, 0.31794872],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.2460733 , 0.7539267 ],\n",
       "       [0.77586207, 0.22413793],\n",
       "       [0.08152174, 0.91847826],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83957219, 0.16042781],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.13227513, 0.86772487],\n",
       "       [0.01587302, 0.98412698],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.89349112, 0.10650888],\n",
       "       [0.15151515, 0.84848485],\n",
       "       [0.96335079, 0.03664921],\n",
       "       [0.00985222, 0.99014778],\n",
       "       [0.65294118, 0.34705882],\n",
       "       [0.07317073, 0.92682927],\n",
       "       [0.99421965, 0.00578035],\n",
       "       [0.76502732, 0.23497268],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92      , 0.08      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.30726257, 0.69273743],\n",
       "       [0.9893617 , 0.0106383 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85326087, 0.14673913],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.77094972, 0.22905028],\n",
       "       [0.96236559, 0.03763441],\n",
       "       [1.        , 0.        ],\n",
       "       [0.66842105, 0.33157895],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.        , 1.        ],\n",
       "       [0.91525424, 0.08474576],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.90960452, 0.09039548],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.74860335, 0.25139665],\n",
       "       [0.09      , 0.91      ],\n",
       "       [0.48275862, 0.51724138],\n",
       "       [0.18617021, 0.81382979],\n",
       "       [0.        , 1.        ],\n",
       "       [0.875     , 0.125     ],\n",
       "       [0.82417582, 0.17582418],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01734104, 0.98265896],\n",
       "       [0.94565217, 0.05434783],\n",
       "       [0.98342541, 0.01657459],\n",
       "       [1.        , 0.        ],\n",
       "       [0.4516129 , 0.5483871 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98930481, 0.01069519],\n",
       "       [0.02604167, 0.97395833],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97206704, 0.02793296],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07978723, 0.92021277],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01142857, 0.98857143],\n",
       "       [1.        , 0.        ],\n",
       "       [0.16931217, 0.83068783],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.32786885, 0.67213115],\n",
       "       [0.11176471, 0.88823529],\n",
       "       [0.25      , 0.75      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9939759 , 0.0060241 ],\n",
       "       [0.24725275, 0.75274725],\n",
       "       [0.99441341, 0.00558659],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9673913 , 0.0326087 ],\n",
       "       [0.3248731 , 0.6751269 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00558659, 0.99441341],\n",
       "       [0.99465241, 0.00534759],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02873563, 0.97126437],\n",
       "       [0.98888889, 0.01111111],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02380952, 0.97619048],\n",
       "       [0.65027322, 0.34972678]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### the decision function for each training instance \n",
    "\n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Patches and Random Subspeace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling both instance and features is called Random Patches method\n",
    "\n",
    "# sampling features is Random Subspaces method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "\n",
    "# RandomForestClassifier has almost all the params \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1) \n",
    "rnd_clf.fit(X_train, y_train) \n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitter = \"random\" : feature select when growing trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  random thershold for each tree \n",
    "\n",
    "# API: ExtraTreesClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09854232652974981\n",
      "sepal width (cm) 0.023494142693854532\n",
      "petal length (cm) 0.4225590684351814\n",
      "petal width (cm) 0.45540446234121446\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "iris = load_iris() \n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost \n",
    "\n",
    "# Gradient Boosting \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost \n",
    "\n",
    "# squential training with instance weight updates \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMME \n",
    "\n",
    "# Adaboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "\n",
    "ada_clf = AdaBoostClassifier( \n",
    "    DecisionTreeClassifier(max_depth = 1), n_estimators = 200,\n",
    "    algorithm = \"SAMME.R\", learning_rate = 0.5\n",
    ")\n",
    "\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting \n",
    "\n",
    "# Sequentially fit by the residual errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "\n",
    "gdrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gdrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning rate \n",
    "\n",
    "# control the contribution of each tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " ### early stopping \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=46)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a second model using the trees with the min mse \n",
    "\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y) \n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train) \n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "         for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors) \n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators) \n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual early stopping, stop when validation stop improving \n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True) \n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0 \n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators \n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val) \n",
    "    val_error = mean_squared_error(y_val, y_pred) \n",
    "    if val_error < min_val_error: \n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0 \n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also could change the training instances fraction \n",
    "\n",
    "# using subsample = 0.25 e.g.\n",
    "\n",
    "# called Stochastic Gradient Boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model to do the aggregattion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could use open source software: brew  https://github.com/viisar/brew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
